[package]
name = "charabia"
version = "0.8.5"
license = "MIT"
authors = ["Many <many@meilisearch.com>"]
edition = "2021"
description = "A simple library to detect the language, tokenize the text and normalize the tokens"
documentation = "https://docs.rs/charabia"
repository = "https://github.com/meilisearch/charabia"
keywords = ["segmenter", "tokenizer", "normalize", "language"]
categories = ["text-processing"]
exclude = ["dictionaries/txt/thai/words.txt"]

[dependencies]
aho-corasick = "1.0.1"
cow-utils = "0.1"
csv = "1.2.1"
deunicode = "1.3.3"
either = "1.8.1"
finl_unicode = { version= "1.2.0", optional = true }
fst = "0.4"
jieba-rs = { version = "0.6", optional = true }
once_cell = "1.17.1"
serde = "1.0"
slice-group-by = "0.3.0"
whatlang = "0.16.2"
lindera-core = { version = "0.27.2", git = "https://github.com/kanarikanaru/lindera.git", branch = "meili_1.6.1" }
lindera-dictionary = { version = "0.27.2", git = "https://github.com/kanarikanaru/lindera.git", branch = "meili_1.6.1" }
lindera-tokenizer = { version = "0.27.2", git = "https://github.com/kanarikanaru/lindera.git", branch = "meili_1.6.1", default-features = false, optional = true }
lindera-ipadic-neologd = { version = "0.27.2", git = "https://github.com/kanarikanaru/lindera.git", branch = "meili_1.6.1" }
pinyin = { version = "0.9", default-features = false, features = [
  "with_tone",
], optional = true }
wana_kana = { version = "3.0.0", git = "https://github.com/kanarikanaru/wana_kana_rust.git", branch = "meilisearch", optional = true }
unicode-normalization = "0.1.22"
irg-kvariants = "0.1.0"
litemap = "0.6.1"
zerovec = "0.9.3"
icu = { version = "1.3.0", features = ["serde"] , optional = true }
icu_provider_blob = { version = "1.3.0", optional = true }
icu_provider = { version = "1.3.0", features = ["sync"], optional = true }

[features]
default = ["hebrew", "japanese", "thai", "korean", "greek", "latin-camelcase", "latin-snakecase", "khmer"]

# allow chinese specialized tokenization
chinese = ["dep:pinyin", "dep:jieba-rs"]

# allow hebrew specialized tokenization
hebrew = []

# allow japanese specialized tokenization
japanese = ["japanese-segmentation-ipadic-neologd", "japanese-transliteration"]
japanese-segmentation-ipadic = ["lindera-tokenizer/ipadic", "lindera-tokenizer/ipadic-compress"]
japanese-segmentation-ipadic-neologd = ["lindera-tokenizer/ipadic-neologd", "lindera-tokenizer/ipadic-neologd-compress"]
japanese-transliteration = ["dep:wana_kana"]

# allow korean specialized tokenization
korean = ["lindera-tokenizer/ko-dic", "lindera-tokenizer/ko-dic-compress"]

# allow thai specialized tokenization
thai = []

# allow greek specialized tokenization
greek = []

# allow splitting camelCase latin words
latin-camelcase = ["dep:finl_unicode"]

khmer = ["dep:icu", "dep:icu_provider_blob", "dep:icu_provider"]

# allow splitting snake_case latin words
latin-snakecase = ["dep:finl_unicode"]

[dev-dependencies]
criterion = "0.3"
jemallocator = "0.3.0"
quickcheck = "1"
quickcheck_macros = "1"


[[bench]]
name = "bench"
harness = false
